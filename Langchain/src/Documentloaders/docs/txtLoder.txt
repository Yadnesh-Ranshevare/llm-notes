LangChain is a framework for building applications that use large language models (LLMs) like GPT, Claude, Llama, etc.

Instead of calling a model directly, LangChain helps you chain together many steps—retrieval, reasoning, tools, memory, and more—to create powerful AI applications.

LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.

LangChain agents are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more.

You do not need to know LangGraph for basic LangChain agent usage.

What LangChain Does
LangChain gives you building blocks to create:

Chatbots
RAG systems (Retrieval-Augmented Generation)
Agents that use tools/APIs
Document analyzers
Workflow pipelines
Core Components & Structure
LangChain for JS/TS is modular, offering these building blocks (among others) for constructing LLM-powered apps.

Models — These are wrappers around different LLM providers. You can switch models (OpenAI → Gemini → Llama) without changing your whole code.
Agents — These are “smart workers” powered by an LLM. They can think, decide, and use tools to complete tasks (like calling APIs or searching data).
Tools — external functions or APIs (like retrieving data, calling external services) that agents (or prompts) can use.
Memory & Messaging / Context Handling — This lets your app remember previous messages or state. Useful for chatbots or any “ongoing conversation”.
Structured Output & Streaming / Middleware / Guardrails — support for structured responses (JSON response, stream responses token-by-token, etc) and safe, controlled flows.